# Azure-Web-Scraper-Tool

This project will guide you building a tool that will analyze data extracted from the web by an Azure Functions using Azure's powerful AI services.

For this example, we want to create an AI-infused search index of training courses offered by [IAEA](https://www.iaea.org/projects/coordinated-research-projects). In addition we'll create a Power BI report to get more insights about the extracted content using the Azure Search knowledge store.

![achitecture](./img/architecture.PNG)

## Table of Contents

- [Azure-Web-Scraper-Tool](#azure-web-scraper-tool)
  - [Requirements](#requirements)
  - [Create a custom web search engine](#create-a-custom-web-search-engine)
  - [Build a Web Scraper using Azure Functions](#build-a-web-scraper-using-azure-functions)
    - [Create an output storage for the function](#create-an-output-storage-for-the-function)
    - [Configuration of the output](#configuration-of-the-output)
    - [Add libraries](#add-libraries)
    - [Build the function](#build-the-function)
    - [Test your app](#test-your-app)
    - [Publish the function to Azure](#publish-the-function-to-azure)
  - [Create an indexer with Azure Search](#create-an-indexer-with-azure-search)
    - [Connect to your data](#connect-to-your-data)
    - [Add Cognitive Skills](#add-cognitive-skills)
    - [Create an index](#create-an-index)
    - [Create an indexer](#create-an-indexer)
    - [Test the search service](#test-the-search-service)
  - [Visualize in Power BI](#visualize-in-power-bi)

## Requirements

- [An Azure subscription](https://azure.microsoft.com/en-gb/free/search/?OCID=AID2200274_SEM_c9311c296c7b119478c87d287eb07988:G:s&ef_id=c9311c296c7b119478c87d287eb07988:G:s&msclkid=c9311c296c7b119478c87d287eb07988)
- [Visual Studio Code](https://code.visualstudio.com/Download)
- [Azure Functions for VS Code](https://marketplace.visualstudio.com/items?itemName=ms-azuretools.vscode-azurefunctions)
- [Python 3.9](https://www.python.org/downloads/release/python-390/)
- [Azure Storage explorer](https://azure.microsoft.com/en-us/features/storage-explorer/#overview)
- [Power BI Desktop](https://powerbi.microsoft.com/en-us/downloads/)
- [Postman application](https://www.postman.com/downloads/)

## Create a custom web search engine

To get the most relevant web pages we want to scrap for a given key words, we are going to use the Bing Search API with customized settings. It will give us a list of web pages that we are going to use later in our web scraper.

- In the Azure portal, deploy a [Bing Custom Search](https://docs.microsoft.com/en-us/bing/search-apis/bing-web-search/create-bing-search-service-resource) resource. For the lab purpose, you can chose the Standard S1 pricing tier.

![search resource](./img/custom-search.png)

- Access the resource. In `Keys and Endpoint`, copy one of the key and save it for later.

- Login to https://www.customsearch.ai/.
- Select the search instance you just created.
- In the `Search Experience` tab, add the list of websites you want to explore. Select **Include Subpages** as we want to get a list of children URLs  from this page - https://www.iaea.org/projects/crp/[id-of-a-crp].

![custom search portal](./img/custom-search-portal.png)

- On the right part of the portal, you can make some tests and search for key words.
- You can then publish the API.
- In the `Production` tab, go to `Endpoints` and save the custom configuration ID.

## Build a Web Scraper using Azure Functions

- In the Azure portal, deploy a Function App. Publish it as code, choose the Python runtime stack and select the Python version installed on your machine.
- Open Visual Studio Code. Go to the Azure tab or press Ctrl + Shift + A.
- In Functions, select `Create new project`.
  - Select a folder to store the code locally;
  - Language: Python
  - Select your Python interpreter
  - Trigger: HTTP trigger
  - Give a name to the function
  - Authorization level: anonymous

### Create an output storage for the function

To store the result generated by the function, create a [storage account](https://ms.portal.azure.com/#create/Microsoft.StorageAccount-ARM) in the Azure portal.

Change the redundancy for LRS and just keep the default settings.

### Configuration of the output

Open the `function.json` file. This is the place to configure the *bindings* of our Azure function: the trigger, inputs and outputs. The HTTP trigger is already configured for us. Now, we need to configure an [output](https://docs.microsoft.com/en-us/azure/azure-functions/functions-bindings-storage-blob-output?tabs=python) to simply store our scraped data within the blob storage we created.

Add this json element within the bindings list:

```json
    {
      "name": "outputblob",
      "type": "blob",
      "dataType": "binary",
      "path": "webdata/{DateTime}/webdata-{rand-guid}.json",
      "connection": "StorageConnectionString",
      "direction": "out"
    }
```

To store the data, we use a unique path based on a GUID coming from a [binding expression](https://docs.microsoft.com/en-us/azure/azure-functions/functions-bindings-expressions-patterns), sorted by a folder named with datetime.

The connection string to connect to the blob will be added in the app settings of the function. It is a good way to store system variables and inject secrets within the app.

In the portal, access your function app. In the side menu, go to configuration. Add a new application setting, call it `StorageConnectionString` and paste the storage connection string as the value.

For our local development, we will also add this value in the `local.settings.json` file.

### Add libraries

To create our web scraper, we will need to use 2 Python libraries, requests for the HTTP calls and Beautiful Soup to parse the HTML.

Access `requirements.txt`:  This file contains the list of Python packages that will be installed by the system when the function is deployed on Azure. Add these 2 lines:

``` txt
requests
beautifulsoup4
```

### Build the function

Finally we will add the code logic within the function. By default, the runtime expects the method to be implemented as a global method called `main()` in the `__init__.py` file.

Copy the content of [\__init__.py](./python_function/GetWebData/__init__.py) within your file. Add the environment variables `SubscriptionKey`  and `ConfigId` in your `local.settings.json` file and in the Azure appsettings, as we have done previously for the connection string. You can see that these variables are consumed like any Python environment variables line 45 and 46 of the script.

### Test your app

To run the function locally, press `F5` and observe the result in VS Code's terminal. The function local endpoint will be displayed, typically `http://localhost:7071/api/[NAME_OF_THE_FUNCTION]`. You can now test it by using Postman or the tool of your choice. We don't need any authentication header as we chose the anonymous configuration.

![vs code console](./img/function_local_test.png)

Open the Azure Storage Explorer, connect to the Azure storage that was configured in your function's local settings and check if a new json file with the website content has been created.

### Publish the function to Azure

In Visual Studio Code, go back to the Azure tab (`Ctrl + Shift + A`) and in the Azure Function area, if you're not yet logged in, choose `Sign into Azure...`. Follow the sign in steps. After you have successfully signed in, you should see your subscription and the Azure Functions resource previously deployed.

Open the command pallet (`Ctrl + Shift + P`) and enter `Azure Functions: Deploy to function app`. Select your subscription and the function app you want to deploy to.

If you go back to the Azure portal, you can now see that the function has been deployed. You can get the function URL from here, that will have the format `https://[FUNCTION_APP_INSTANCE].azurewebsites.net/api/[FUNCTION_NAME]`

## Create an indexer with Azure Search

In the Azure portal, deploy an Azure Cognitive Search resource. You can choose the free or basic tier. Go to the deployed instance and go through the following steps.

### Connect to your data

Select `Import Data`.

- **Data source**: Azure Blob Storage
- **Data to extract**: Content and Metadata
- **Parsing mode**: Default
- **Connection string**:  Choose an existing connection and select the storage we previously used

![search data source](./img/connect-data.PNG)

### Add Cognitive Skills

We are now going to add AI enrichment to our search index. In this lab, we are going to use some buit-in Natural Language Processing skills from Microsoft. You could also add your own [custom skills](https://docs.microsoft.com/en-us/azure/search/cognitive-search-concept-intro) to your Azure Search resource.

![AI enrichment pipeline](./img/cogsearch-architecture.png)

A free Cognitive Services resource should be available. Select it or create a new one.

Select the enrichments you want to add to your Search indexer.

![data enrichment](./img/data-enrichment.PNG)

Save the enrichments to a knowledge store. It will save the enrichements' result within a table storage and enable us to reuse the content in other applications, such as in Power BI reports.

Copy the Power BI parameters and download the Power BI template for later.

### Create an index

Customize the index according to your requirements. By default, content is searchable and retrievable. The metadata_storage_path is used as the key field, which is the unique identifier for the documents within the index.

![index](./img/index.PNG)

### Create an indexer

Finally, create an indexer. You can schedule it once for now.

> For this lab, we have been using the Azure portal interface to build our Cognitive Search indexer. It is a good way to get familiar with the service and its concepts of data source, enrichment, knowledge store, index and indexer. However this approach makes update to the index and indexers uneasy. For this reason, a programmatic approach is recommended for the projects. You can find a template of the API calls necessary for the preceding steps in this [repo](./azure_search_api). The collection can be imported in Postman, where the `Cognitive Search Env` environment needs to be configured.

### Test the search service

In the portal, in the indexer tab from the Search instance, check if the indexer has the correct number of succeeded documents and does not have any errors or warnings.

Then, select the index tab. You can test the search service directly from the portal, or you can copy the request URL, get the admin key and execute the requests from Postman. 

## Visualize in Power BI

In Power BI Desktop, open the [Power BI Cognitive Search template](./power_bi/). At the opening, fill the form that shows up with the values you get when creating the knowledge store.

![pbi](./img/power_bi_template.PNG)

Update the visuals and data according to your needs.